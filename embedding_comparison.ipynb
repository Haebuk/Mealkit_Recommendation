{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCSE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu113'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2021 13:33:25 - INFO - simcse.tool -   Use `cls_before_pooler` for unsupervised models. If you want to use other pooling policy, specify `pooler` argument.\n"
     ]
    }
   ],
   "source": [
    "from simcse import SimCSE\n",
    "model = SimCSE('princeton-nlp/unsup-simcse-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"한 남자가 스케이트 보드를 탄다.\" is: 0.984\n",
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"할아버지가 식사를 하신다.\" is: 0.969\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"저기 스케이트보드를 타는 학생이 있다.\",\n",
    "    \"한 남자가 스케이트 보드를 탄다.\",\n",
    "    \"할아버지가 식사를 하신다.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-0.0342,  0.0079,  0.0563,  ..., -0.0561,  0.0047,  0.0192],\n",
      "        [-0.0190, -0.0207,  0.0216,  ..., -0.0090, -0.0134,  0.0204],\n",
      "        [ 0.0161, -0.0826,  0.0426,  ..., -0.0160, -0.0296, -0.0425]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\n",
    "    \"저기 스케이트보드를 타는 학생이 있다.\",\n",
    "    \"한 남자가 스케이트 보드를 탄다.\",\n",
    "    \"할아버지가 식사를 하신다.\"\n",
    "]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"한 남자가 스케이트 보드를 탄다.\" is: 0.758\n",
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"할아버지가 식사를 하신다.\" is: 0.134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cosine_sim_0_1 = 1 - cosine(sentence_embeddings[0], sentence_embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(sentence_embeddings[0], sentence_embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (sentences[0], sentences[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (sentences[0], sentences[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KR-SBERT\n",
    "- pre-required https://github.com/snunlp/KR-SBERT files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2021 13:33:48 - INFO - sentence_transformers.SentenceTransformer -   Load pretrained SentenceTransformer: KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS\n",
      "11/04/2021 13:33:53 - INFO - sentence_transformers.SentenceTransformer -   Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e607e9271b4dbe8eba9884935d226e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"한 남자가 스케이트보드를 탄다.\" is: 0.793\n",
      "Cosine similarity between \"저기 스케이트보드를 타는 학생이 있다.\" and \"할아버지가 식사를 하신다.\" is: 0.186\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('KR-SBERT/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"저기 스케이트보드를 타는 학생이 있다.\",\n",
    "    \"한 남자가 스케이트보드를 탄다.\",\n",
    "    \"할아버지가 식사를 하신다.\"\n",
    "]\n",
    "\n",
    "vectors = model.encode(texts)\n",
    "similarities = cosine_similarity(vectors)\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], similarities[0][1]))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], similarities[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ace3e1ad7af0847684bd297f2968649b4d96ae2f627f328ee2eafd305163f7dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rjs': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
